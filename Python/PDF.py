import os
import subprocess
import requests
import PyPDF2
from sentence_transformers import SentenceTransformer
import faiss
import gradio as gr
import numpy as np
import json
import logging
import urllib3
from typing import List, Tuple  # Ajout√© pour les annotations de type

from ollama import Client  # Assurez-vous que la biblioth√®que Ollama est install√©e

logging.basicConfig(level=logging.ERROR)
logging.getLogger("gradio").setLevel(logging.ERROR)
logging.getLogger("urllib3").setLevel(logging.ERROR)
logging.getLogger("requests").setLevel(logging.ERROR)
logging.getLogger("transformers").setLevel(logging.ERROR)
logging.getLogger("asyncio").setLevel(logging.ERROR)

# Configuration du logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


# Liste des mod√®les disponibles au t√©l√©chargement
available_models = [
    "llama3.1",
    "mistral",
    "starling-lm"
]

# Liste des prompts syst√®me
system_prompts = [
    "",
    "Dans ce formulaire administratif, quels sont les champs obligatoires √† remplir dans ce formulaire administratif, quelles informations pr√©cises sont demand√©es, et quelles sont les √©ventuelles erreurs ou omissions qui pourraient entra√Æner un refus ou un retard dans le traitement de ce document officiel ?",

    "Dans ce livre d'auteur, peux-tu r√©sumer les id√©es principales, th√®mes abord√©s et le message global que l'auteur cherche √† transmettre dans ce livre, en soulignant √©galement les passages cl√©s ou les moments forts qui marquent la progression de l‚Äôintrigue ou de l‚Äôargumentation ?",

    "Dans ce magazine de presse People, quels sujets, c√©l√©brit√©s et √©v√©nements sont mis en avant dans ce num√©ro de magazine People, et comment ces articles couvrent-ils des aspects de la vie personnelle, professionnelle ou publique de ces personnalit√©s connues, avec des anecdotes ou d√©tails particuliers ?",

    "Dans cette documentation technique, quelles sont les √©tapes pr√©cises pour installer, configurer ou d√©panner le produit ou service d√©crit dans cette documentation technique, et quelles recommandations ou pr√©cautions doivent √™tre suivies pour assurer un bon fonctionnement et √©viter les erreurs fr√©quentes ?",

    "Dans cet acte notari√©, quels sont les termes l√©gaux, les clauses et obligations majeures √©nonc√©es dans cet acte notari√©, et quels droits, responsabilit√©s ou protections sp√©cifiques cet acte conf√®re-t-il aux parties concern√©es dans le cadre d‚Äôun contrat ou d‚Äôune transaction juridique ?",

    "Dans ces √©l√©ment de justice, quels sont les faits, les preuves pr√©sent√©es et les arguments des parties dans cette affaire judiciaire, et quelles d√©cisions ou conclusions ont √©t√© rendues par le tribunal, ainsi que les implications juridiques ou p√©nales de ce jugement pour les personnes impliqu√©es ?",

    "Dans ce capture d'√©cra, quels √©l√©ments d'interface, informations ou d√©tails visuels sont pr√©sents dans cette capture d'√©cran, et comment ces √©l√©ments peuvent-ils √™tre interpr√©t√©s pour diagnostiquer un probl√®me technique, analyser une situation ou effectuer une action sp√©cifique ?"
]

# Fonction pour lister les mod√®les disponibles localement via la commande 'ollama list'
def get_installed_models() -> List[str]:
    try:
        models = subprocess.check_output(["ollama", "list"], stderr=subprocess.DEVNULL).decode("utf-8")
        # Filtrer les mod√®les √† exclure comme "failed" ou "aya"
        filtered_models = [
            model.split(":")[0].strip() for model in models.splitlines()
            if not (model.startswith("NAME") or "failed" in model or "aya" in model or "nomic-embed-text" in model or "mxbai-embed-large" in model or "bakllava" in model or "llava" in model or "llava-llama3" in model or "llava-phi3" in model or "minicpm-v" in model)
        ]
        return sorted(filtered_models)
    except subprocess.CalledProcessError as e:
        logging.error(f"Erreur lors de l'ex√©cution de la commande 'ollama list': {e}")
        return []

# Fonction combinant les mod√®les install√©s et disponibles au t√©l√©chargement
def get_models() -> List[str]:
    installed_models = get_installed_models()
    all_models = sorted(set(installed_models + available_models))
    return all_models

# Fonction pour v√©rifier si un mod√®le est install√©
def is_model_installed(model_name: str) -> bool:
    installed_models = get_installed_models()
    return model_name in installed_models

# Fonction pour g√©rer la s√©lection et le t√©l√©chargement des mod√®les
def handle_model_selection(model: str) -> None:
    if not is_model_installed(model):
        logging.info(f"Le mod√®le {model} n'est pas trouv√©. Tentative de t√©l√©chargement...")
        try:
            subprocess.run(["ollama", "pull", model], check=True, text=True)
            logging.info(f"Le mod√®le {model} a √©t√© t√©l√©charg√© et install√© avec succ√®s.")
        except subprocess.CalledProcessError as e:
            error_message = e.stderr.strip() if e.stderr else "Une erreur est survenue lors du t√©l√©chargement du mod√®le."
            logging.error(f"Erreur lors du t√©l√©chargement du mod√®le {model} : {error_message}")
            raise Exception(f"√âchec du t√©l√©chargement du mod√®le {model}. Veuillez essayer de le t√©l√©charger manuellement.")
    else:
        logging.info(f"Le mod√®le {model} est d√©j√† install√©.")

# Fonction pour formater l'historique de chat (utile si vous souhaitez conserver un historique dans le chatbot)
def format_history(msg: str, history: List[Tuple[str, str]], system_prompt: str):
    chat_history = [{"role": "system", "content": system_prompt}]
    for query, response in history:
        chat_history.append({"role": "user", "content": query})
        chat_history.append({"role": "assistant", "content": response})
    chat_history.append({"role": "user", "content": msg})
    return chat_history

# Initialisation du mod√®le de vectorisation avec Sentence Transformers
try:
    embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
    logging.info("Mod√®le SentenceTransformer charg√© avec succ√®s")
except Exception as e:
    logging.error(f"Erreur lors du chargement du mod√®le SentenceTransformer : {str(e)}")
    embedding_model = None  # Assurez-vous de g√©rer ce cas dans les fonctions suivantes

# Fonction pour vectoriser le texte
def vectorize_text(texts: List[str]) -> np.ndarray:
    try:
        logging.info("Vectorisation du texte...")
        vectors = embedding_model.encode(texts, convert_to_numpy=True)
        return vectors
    except Exception as e:
        logging.error(f"Erreur lors de la vectorisation : {str(e)}")
        return np.array([])  # Retourne un array vide en cas d'erreur

# Cr√©ation de l'index FAISS
def create_faiss_index(vectors: np.ndarray):
    try:
        d = vectors.shape[1]
        index = faiss.IndexFlatL2(d)
        index.add(vectors)
        logging.info("Index FAISS cr√©√© avec succ√®s")
        return index
    except Exception as e:
        logging.error(f"Erreur lors de la cr√©ation de l'index FAISS : {str(e)}")
        return None

# Fonction pour extraire le texte d'un PDF
def extract_text_from_pdf(pdf_path: str) -> str:
    try:
        with open(pdf_path, 'rb') as file:
            reader = PyPDF2.PdfReader(file)
            text = "".join(page.extract_text() for page in reader.pages if page.extract_text() is not None)
        logging.info("Extraction du texte r√©ussie")
        return text
    except Exception as e:
        logging.error(f"Erreur lors de l'extraction du texte PDF : {str(e)}")
        return f"Erreur lors de la lecture du PDF : {str(e)}"

# Fonction pour interroger l'API d'Ollama avec le mod√®le et le prompt s√©lectionn√©s
OLLAMA_API_URL = "http://localhost:11434/api/generate"

def query_ollama(prompt: str, context: str, temperature: float, top_k: int, top_p: float, 
                presence_penalty: float, frequency_penalty: float, repeat_penalty: float, 
                num_keep: int, num_predict: int, num_ctx: int, model_selection: str, 
                system_prompt: str) -> str:
    try:
        logging.info("Interrogation de l'API Ollama...")

        final_prompt = f"{system_prompt}\n\n### Contexte :\n{context}\n\n### Question :\n{prompt}"

        headers = {
            "Content-Type": "application/json"
        }
        data = {
            "model": model_selection,
            "prompt": final_prompt,
            "options": {
                "temperature": temperature,
                "top_k": top_k,
                "top_p": top_p,
                "presence_penalty": presence_penalty,
                "frequency_penalty": frequency_penalty,
                "repeat_penalty": repeat_penalty,
                "num_keep": num_keep,
                "num_predict": num_predict,
                "num_ctx": num_ctx
            }
        }
        json_data = json.dumps(data, ensure_ascii=False)
        logging.info(f"JSON data sent to Ollama API: {json_data}")

        response = requests.post(OLLAMA_API_URL, data=json_data, headers=headers, timeout=10)
        response.raise_for_status()

        response_content = response.content.decode('utf-8', errors='ignore').strip()
        logging.info(f"Raw response (limited to 500 chars): {response_content[:500]}")

        # Extraction des fragments JSON
        import re
        json_fragments = re.findall(r'({.*?})', response_content)

        final_response = ""
        done = False
        for fragment in json_fragments:
            try:
                fragment_json = json.loads(fragment)
                final_response += fragment_json.get('response', "")
                if fragment_json.get('done', False):
                    done = True
                    break
            except json.JSONDecodeError as e:
                logging.error(f"JSONDecodeError while processing fragment: {fragment[:100]} - {str(e)}")
                continue

        if not done:
            logging.warning("R√©ponse incompl√®te, certains fragments peuvent manquer.")

        return final_response if final_response else "Aucune r√©ponse g√©n√©r√©e"

    except requests.RequestException as e:
        logging.error(f"Erreur lors de l'interrogation de l'API Ollama : {str(e)}")
        return f"Erreur lors de l'interrogation de l'API Ollama : {str(e)}"

# Variables globales pour stocker les documents vectoris√©s et l'index FAISS
document_texts: List[str] = []
faiss_index = None

# Fonction pour ajouter un PDF et le vectoriser automatiquement
def add_pdf_and_vectorize(pdf) -> str:
    global document_texts, faiss_index
    try:
        text = extract_text_from_pdf(pdf.name)
        if isinstance(text, str) and text.startswith("Erreur"):
            return text
        document_texts.append(text)
        vector = vectorize_text([text])
        if vector.size == 0:
            return "Erreur lors de la vectorisation du document"

        vector = np.array(vector)
        if vector.ndim == 1:
            vector = vector.reshape(1, -1)

        if faiss_index is None:
            faiss_index = create_faiss_index(vector)
            if faiss_index is None:
                return "Erreur lors de la cr√©ation de l'index FAISS"
        else:
            faiss_index.add(vector)

        logging.info(f"Document {pdf.name} ajout√© et vectoris√©.")
        return f"Document {pdf.name} ajout√© et vectoris√©."
    except Exception as e:
        logging.error(f"Erreur lors de l'ajout du PDF : {str(e)}")
        return f"Erreur lors de l'ajout du PDF : {str(e)}"

# Fonction pour r√©pondre √† une question
def answer_question(question: str, temperature: float, top_k: int, top_p: float, 
                   presence_penalty: float, frequency_penalty: float, repeat_penalty: float, 
                   num_keep: int, num_predict: int, num_ctx: int, model_selection: str, 
                   system_prompt: str) -> str:
    global faiss_index, document_texts
    try:
        if faiss_index is None or not document_texts:
            return "Aucun document ajout√©."

        question_vector = vectorize_text([question])
        if question_vector.size == 0:
            return "Erreur lors de la vectorisation de la question"
        question_vector = question_vector[0].reshape(1, -1)

        D, I = faiss_index.search(question_vector, k=1)
        if I[0][0] == -1:
            return "Aucun contexte trouv√© pour la question pos√©e."
        context = document_texts[I[0][0]]

        return query_ollama(question, context, temperature, top_k, top_p, presence_penalty, 
                            frequency_penalty, repeat_penalty, num_keep, num_predict, 
                            num_ctx, model_selection, system_prompt)
    except Exception as e:
        logging.error(f"Erreur lors de la r√©ponse √† la question : {str(e)}")
        return f"Erreur lors de la r√©ponse √† la question : {str(e)}"

# Interface Gradio
def launch_interface():
    try:
        models = get_models()

        with gr.Blocks() as interface:
            with gr.Row():
                with gr.Column(scale=1):
                    model_selection = gr.Dropdown(
                        choices=models,
                        label="üíª S√©lection du mod√®le IA",
                        value="llama3.1" if "llama3.1" in models else models[0],
                        interactive=True,
                        elem_id="model-selection"
                    )
                with gr.Column(scale=3):
                    system_prompt = gr.Dropdown(
                        choices=system_prompts,
                        label="üìÉ S√©lection du Prompt Syst√®me",
                        value=system_prompts[0],
                        interactive=True
                    )

            with gr.Accordion(" üéõÔ∏è Param√®tres avanc√©s", open=False):
                gr.Markdown("üî© R√©glages des options avec Ollama ü¶ô")
                with gr.Row():
                    with gr.Column(scale=1):
                        temperature = gr.Slider(
                            minimum=0.5, maximum=1.5, step=0.1, label="üéöÔ∏è Temperature", value=0.8,
                            info="D√©finit la cr√©ativit√© du mod√®le. Valeurs plus √©lev√©es produisent des r√©ponses plus vari√©es et impr√©visibles, tandis que des valeurs plus basses sont plus conservatrices."
                        )
                    with gr.Column(scale=1):
                        top_k = gr.Slider(
                            minimum=20, maximum=60, step=10, label="üéöÔ∏è Top K", value=40,
                            info="Nombre maximum de mots parmi lesquels choisir pour chaque √©tape. Plus la valeur est √©lev√©e, plus la diversit√© des r√©ponses est grande."
                        )
                    with gr.Column(scale=1):
                        top_p = gr.Slider(
                            minimum=0.85, maximum=0.98, step=0.01, label="üéöÔ∏è Top P", value=0.92,
                            info="Limite la s√©lection aux mots les plus probables jusqu'√† ce que leur somme atteigne une certaine probabilit√©, favorisant la diversit√©."
                        )

                with gr.Row():
                    with gr.Column(scale=1):
                        presence_penalty = gr.Slider(
                            minimum=0.1, maximum=0.5, step=0.1, label="üéöÔ∏è Presence Penalty", value=0.5,
                            info="P√©nalise les mots nouvellement introduits dans le texte, r√©duisant leur probabilit√© d'√™tre r√©p√©t√©s pour am√©liorer la coh√©rence globale."
                        )
                    with gr.Column(scale=1):
                        frequency_penalty = gr.Slider(
                            minimum=0.0, maximum=2.0, step=0.1, label="üéöÔ∏è Frequency Penalty", value=1.0,
                            info="Diminue la probabilit√© de r√©utiliser les mots d√©j√† apparus dans le texte, favorisant une plus grande diversit√© de vocabulaire."
                        )
                    with gr.Column(scale=1):
                        repeat_penalty = gr.Slider(
                            minimum=1.0, maximum=2.0, step=0.1, label="üéöÔ∏è Repeat Penalty", value=1.2,
                            info="P√©nalise les r√©p√©titions excessives des m√™mes mots dans le texte g√©n√©r√©, am√©liorant ainsi la fluidit√© et la vari√©t√© des phrases."
                        )

                with gr.Row():
                    with gr.Column(scale=1):
                        num_keep = gr.Slider(
                            minimum=1024, maximum=4096, step=1024, label="üéöÔ∏è Num Keep", value=3072,
                            info="Nombre de tokens √† conserver comme contexte pour la g√©n√©ration actuelle, garantissant la coh√©rence avec le contenu initial."
                        )
                    with gr.Column(scale=1):
                        num_predict = gr.Slider(
                            minimum=1024, maximum=6144, step=1024, label="üéöÔ∏è Num Predict", value=4096,
                            info="Nombre total de tokens que le mod√®le est cens√© g√©n√©rer. Plus la valeur est √©lev√©e, plus la r√©ponse sera longue."
                        )
                    with gr.Column(scale=1):
                        num_ctx = gr.Slider(
                            minimum=1024, maximum=8192, step=1024, label="üéöÔ∏è Num Context", value=7168,
                            info="Taille maximale du contexte pris en compte pour chaque g√©n√©ration, influen√ßant la pertinence des r√©ponses bas√©es sur l'entr√©e fournie."
                        )


            with gr.Row():
                with gr.Column(scale=1):
                    pdf_input = gr.File(label="Ajoutez un fichier PDF", type="filepath")
                    output_add_pdf = gr.Textbox(label="üì¶ Vecteurs du PDF", interactive=False)
                    question_input = gr.Textbox(
                        lines=7, 
                        label="‚ùì Posez votre question", 
                        placeholder="Entrez une question ici..."
                    )
                with gr.Column(scale=2):
                    answer_output = gr.Textbox(lines=20, label="üìù R√©ponse", interactive=False, elem_id="R√©ponse")
                    submit_question_button = gr.Button("üó®Ô∏è Envoyer")
                    
            # Actions li√©es aux interactions
            pdf_input.change(fn=add_pdf_and_vectorize, inputs=pdf_input, outputs=output_add_pdf)
            submit_question_button.click(
                fn=answer_question,
                inputs=[
                    question_input, temperature, top_k, top_p, presence_penalty, 
                    frequency_penalty, repeat_penalty, num_keep, num_predict, 
                    num_ctx, model_selection, system_prompt
                ],
                outputs=answer_output
            )

            # Action lors de la s√©lection d'un mod√®le : V√©rifier et t√©l√©charger si n√©cessaire
            model_selection.change(
                fn=lambda model: handle_model_selection(model) or gr.update(),
                inputs=model_selection,
                outputs=None,
                queue=False  # Ex√©cuter imm√©diatement sans file d'attente
            )

        logging.info("Interface Gradio lanc√©e avec succ√®s.")
        return interface
    except Exception as e:
        logging.error(f"Erreur lors du lancement de l'interface : {str(e)}")
        raise

if __name__ == "__main__":
    interface = launch_interface()
    interface.launch(share=True)

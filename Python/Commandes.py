import gradio as gr
import pyttsx3
import speech_recognition as sr
import subprocess
import threading
import requests
import json
import os
from typing import List, Tuple
import ctypes

# Utilisation de constantes pour Ã©viter les recomptages inutiles
AVAILABLE_MODELS = [
    "llama3.1", "gemma2", "mistral-nemo", "starling-lm"
]

def is_admin():
    try:
        return ctypes.windll.shell32.IsUserAnAdmin()
    except:
        return False

def open_task_manager():
    if is_admin():
        # ExÃ©cution du gestionnaire des tÃ¢ches directement si dÃ©jÃ  en mode admin
        return (subprocess.Popen('taskmgr.exe'), "Le gestionnaire des tÃ¢ches a Ã©tÃ© ouvert.")
    else:
        # Demande d'Ã©lÃ©vation des droits si pas en mode admin
        ctypes.windll.shell32.ShellExecuteW(None, "runas", "taskmgr.exe", None, None, 1)
        return (None, "Demande d'Ã©lÃ©vation des droits envoyÃ©e pour ouvrir le gestionnaire des tÃ¢ches.")

def open_folder(folder_path):
    resolved_path = os.path.expandvars(folder_path)
    try:
        subprocess.Popen(['explorer', resolved_path])
        return "Dossier ouvert."
    except Exception as e:
        return f"Erreur lors de l'ouverture du dossier : {e}"

COMMANDS = {
# commande test
    'bonjour': lambda: (None, "Comment puis-je vous aider aujourd'hui?"),

# commandes des applications    
    'ouvre la calculette': lambda: (subprocess.Popen('calc.exe'), "La calculette a Ã©tÃ© ouverte."),
    'ouvre le bloc-notes': lambda: (subprocess.Popen('notepad.exe'), "Le bloc-notes a Ã©tÃ© ouvert."),
    'ouvre outlook': lambda: (subprocess.Popen(r'C:\Program Files\Microsoft Office\root\Office16\OUTLOOK.EXE'), "outlook a Ã©tÃ© ouvert."),
    'mets de la musique': lambda: (play_music(), "La playlist a Ã©tÃ© ouverte dans VLC."),

# commandes Windows
    'ouvre le gestionnaire des tÃ¢ches': open_task_manager,    
    'ouvre le panneau des configurations': lambda: (subprocess.Popen('control.exe')),
    'verrouille la session': lambda: (subprocess.Popen('rundll32.exe user32.dll, LockWorkStation')),
    'ouvre les tÃ©lÃ©chargements': lambda: (None, open_folder(r"%USERPROFILE%\Downloads")),
    'ouvre le dossier des documents': lambda: (None, open_folder(r"%USERPROFILE%\Documents")),
    'ouvre le dossier des images': lambda: (None, open_folder(r"%USERPROFILE%\Pictures")),
    'ouvre le dossier vidÃ©o': lambda: (None, open_folder(r"%USERPROFILE%\Videos")),
    'ouvre le dossier utilisateur': lambda: (None, open_folder(r"%USERPROFILE%")),
# commandes Web
    'ouvre youtube': lambda: (subprocess.Popen('start iexplore.exe https://www.youtube.com/feed/subscriptions/', shell=True), "youtube a Ã©tÃ© ouvert dans le navigateur."),
    'ouvre facebook': lambda: (subprocess.Popen('start iexplore.exe https://www.facebook.com/?sk=h_chr', shell=True), "facebook a Ã©tÃ© ouvert dans le navigateur."),
    'ouvre plex': lambda: (subprocess.Popen('start iexplore.exe https://app.plex.tv/desktop/#!/', shell=True), "plex a Ã©tÃ© ouvert dans le navigateur."),

# commandes tÃ©lÃ©
    'ouvre france 24': lambda: (subprocess.Popen('start iexplore.exe https://www.youtube.com/embed/l8PMl7tUDIE?autoplay=1&mute=1', shell=True), "france 24 a Ã©tÃ© ouverte dans le navigateur."),
    'ouvre cnews': lambda: (subprocess.Popen('start iexplore.exe https://www.dailymotion.com/embed/video/x3b68jn?autoplay=1', shell=True), "cnews a Ã©tÃ© ouverte dans le navigateur."),
}

def play_music():
    playlist_path = os.path.expandvars(r'%userprofile%\Music\playlist.m3u')
    vlc_path = r'C:\Program Files\VideoLAN\VLC\vlc.exe'
    if os.path.isfile(playlist_path) and os.path.isfile(vlc_path):
        subprocess.Popen([vlc_path, playlist_path])
    else:
        raise FileNotFoundError("Le fichier de playlist ou VLC n'a pas Ã©tÃ© trouvÃ©.")

def remove_repeated_phrases(text: str) -> str:
    seen = set()
    result = []
    for word in text.split():
        if word not in seen:
            seen.add(word)
            result.append(word)
    return ' '.join(result)

def generate_response(msg: str, history: List[Tuple[str, str]], model: str,
                      temperature=0.8, num_predict=1024, top_k=40, top_p=0.92,
                      presence_penalty=0.5, repeat_penalty=1.5, frequency_penalty=1.8, num_ctx=7168, num_keep=3072):
    chat_history = [{"role": "user", "content": msg}]
    for query, response in history:
        chat_history.extend([
            {"role": "user", "content": query},
            {"role": "assistant", "content": response}
        ])
    if not is_model_installed(model):
        handle_model_selection(model)
        if not is_model_installed(model):
            AVAILABLE_MODELS.remove(model)
            return f"Erreur : Le modÃ¨le {model} n'a pas pu Ãªtre tÃ©lÃ©chargÃ©."

    try:
        response = requests.post(
            "http://localhost:11434/api/chat",
            json={
                "model": model,
                "messages": chat_history,
                "options": {
                    "temperature": temperature,
                    "top_k": top_k,
                    "top_p": top_p,
                    "presence_penalty": presence_penalty,
                    "repeat_penalty": repeat_penalty,
                    "frequency_penalty": frequency_penalty,
                    "num_predict": num_predict,
                    "num_ctx": num_ctx,
                    "num_keep": num_keep
                }
            },
            timeout=60,
            stream=True
        )
        response.raise_for_status()

        full_content = ""
        for line in response.iter_lines(decode_unicode=True):
            if line:
                try:
                    part = json.loads(line)
                    if 'message' in part and 'content' in part['message']:
                        full_content += part['message']['content']
                except json.JSONDecodeError:
                    return f"Erreur lors de la rÃ©ponse du modÃ¨le : {line}"

        return remove_repeated_phrases(full_content.strip())

    except requests.RequestException as e:
        return f"Erreur lors de la gÃ©nÃ©ration de la rÃ©ponse : {str(e)}"


def get_installed_models():
    try:
        models = subprocess.check_output(["ollama", "list"], stderr=subprocess.DEVNULL, universal_newlines=True)
        return [model.split(":")[0].strip() for model in models.splitlines() if not model.startswith("NAME")]
    except subprocess.CalledProcessError:
        return []

def is_model_installed(model_name):
    return model_name in get_installed_models()

def handle_model_selection(model):
    if not is_model_installed(model):
        try:
            subprocess.run(["ollama", "pull", model], check=True)
        except subprocess.CalledProcessError as e:
            print(f"Erreur lors du tÃ©lÃ©chargement du modÃ¨le {model}: {e}")

def get_available_voices():
    engine = pyttsx3.init()
    voices = engine.getProperty('voices')
    return [voice.name for voice in voices]

def set_audio_params(engine, volume=1.0, rate=200, voice_name='Hortense'):
    engine.setProperty('volume', volume / 100.0)
    engine.setProperty('rate', rate)
    for voice in engine.getProperty('voices'):
        if voice_name.lower() in voice.name.lower():
            engine.setProperty('voice', voice.id)
            break

def execute_command(command: str):
    for cmd, action in COMMANDS.items():
        if cmd in command:
            try:
                result, message = action()
                return message
            except Exception as e:
                return f"Erreur lors de l'exÃ©cution de la commande : {str(e)}"
    return None  # Commande non reconnue
def record_and_send(model, language="fr-FR", mic_index=0, voice_name='Hortense', volume=50, rate=200, history=[], stop_signal=None):
    recognizer = sr.Recognizer()
    engine = pyttsx3.init()
    set_audio_params(engine, volume=volume, rate=rate, voice_name=voice_name)
    microphone = sr.Microphone(device_index=mic_index)
    in_ollama_mode = False

    while not stop_signal[0]:
        with microphone as source:
            print("Enregistrement en cours... Parlez maintenant.")
            try:
                audio = recognizer.listen(source, timeout=5, phrase_time_limit=10)
            except sr.WaitTimeoutError:
                print("Aucun son dÃ©tectÃ©, rÃ©essai...")
                continue
        try:
            print("Analyse de la commande vocale...")
            user_input = recognizer.recognize_google(audio, language=language).lower()
            print(f"Vous avez dit : {user_input}")

            if 'mode au lama' in user_input:
                response = "Passage en mode au Lama, quelle est votre question?"
                in_ollama_mode = True
            elif in_ollama_mode:
                if 'mode commande' in user_input:
                    response = "Fermeture du mode au Lama. Retour en mode commande."
                    in_ollama_mode = False
                else:
                    response = generate_response(user_input, history, model)
                    response += "\nAvez-vous une autre question?"
            else:
                command_response = execute_command(user_input)
                if command_response:
                    response = command_response
                else:
                    response = "DÃ©solÃ©, je n'ai pas compris la commande. Pouvez-vous rÃ©pÃ©ter?"

            print(f"RÃ©ponse : {response}")
            engine.say(response)
            engine.runAndWait()

        except sr.UnknownValueError:
            print("Erreur : La reconnaissance vocale n'a pas compris l'audio.")
        except sr.RequestError as e:
            print(f"Erreur : Impossible de contacter le service de reconnaissance vocale ; {e}")
        except Exception as e:
            print(f"Erreur inattendue : {str(e)}")

def launch_interface():
    with gr.Blocks() as interface:
        with gr.Row():
            with gr.Column():
                model_selection = gr.Dropdown(
                    choices=AVAILABLE_MODELS,
                    label="ğŸ’» SÃ©lection du modÃ¨le IA",
                    value="llama3.1" if "llama3.1" in AVAILABLE_MODELS else AVAILABLE_MODELS[0]
                )
            with gr.Column():
                with gr.Accordion(" ğŸ›ï¸ ParamÃ¨tres avancÃ©s", open=False):
                    gr.Markdown("ğŸ”© RÃ©glages des options avec Ollama ğŸ¦™")
                    mic_list = sr.Microphone.list_microphone_names()
                    mic_choices = [f"{i} {name}" for i, name in enumerate(mic_list)]
                    mic_index = gr.Dropdown(
                        choices=mic_choices,
                        label="ğŸ™ï¸ SÃ©lection du Microphone",
                        value=mic_choices[0] if mic_choices else "0"
                    )
                    language = gr.Dropdown(['fr-FR', 'en-EN'], label="ğŸŒ Langue", value='fr-FR')
                    available_voices = get_available_voices()
                    voice = gr.Dropdown(available_voices, label="ğŸ—£ï¸ Voix", value='Hortense' if 'Hortense' in available_voices else available_voices[0])
                    volume = gr.Slider(minimum=0, maximum=100, step=10, label="ğŸšï¸ Volume", value=50)
                    rate = gr.Slider(50, 300, label="â© Vitesse de la voix", value=200)
                    
                output = gr.Markdown("## Etat du systÃ¨me de commandes", elem_classes="centered")

        with gr.Row():
            start_button = gr.Button("â–¶ï¸ Commencer Ã  parler")
            stop_button = gr.Button("â¹ï¸ ArrÃªter l'enregistrement")

        stop_signal = [False]

        def toggle_recording(model_selection, language, mic_index, voice, volume, rate):
            stop_signal[0] = False
            mic_idx = int(mic_index.split()[0])
            threading.Thread(target=record_and_send, args=(
                model_selection, language, mic_idx, voice, volume, rate, [], stop_signal
            )).start()
            return "## ğŸ”Š Enregistrement en cours..."

        def stop_recording():
            stop_signal[0] = True
            return "## ğŸ”‡ Enregistrement arrÃªtÃ©."

        start_button.click(
            toggle_recording,
            inputs=[model_selection, language, mic_index, voice, volume, rate],
            outputs=[output]
        )
        stop_button.click(stop_recording, inputs=[], outputs=[output])
        gr.Markdown()
        gr.Markdown("# Liste des commandes vocales")
        gr.Markdown()
        gr.Markdown("Pour passer en mode Ollama et discuter : ğŸ“¢ Passage en Mode Ollama")
        gr.Markdown("Pour quitter le mode Ollama et retourner en mode commandse vocales : ğŸ“¢ Retour en mode commande")
        gr.Markdown()
        
        with gr.Row():
            with gr.Column():
                gr.Markdown("**Commandes des applications**")
                gr.Markdown("1. ğŸ“¢ Ouvre la calculette\n2. ğŸ“¢ Ouvre le bloc-notes\n3. ğŸ“¢ Ouvre outlook\n4. ğŸ“¢ Mets de la musique\n5. \n6. \n7. \n8.")
            with gr.Column():
                gr.Markdown("**Commandes du systÃ¨me Windows**")
                gr.Markdown("1. ğŸ“¢ Ouvre le gestionnaire des tÃ¢ches\n2. ğŸ“¢ Ouvre le panneau des configurations\n3. ğŸ“¢ Verrouille la session\n4. ğŸ“¢ Ouvre les tÃ©lÃ©chargements\n5. ğŸ“¢ Ouvre le dossier des documents\n6. ğŸ“¢ Ouvre le dossier des images\n7. ğŸ“¢ Ouvre le dossier vidÃ©o\n8. ğŸ“¢ Ouvre le dossier utilisateur")
            with gr.Column():
                gr.Markdown("**Commandes du navigateur Web**")
                gr.Markdown("1. ğŸ“¢ Ouvre la mosaÃ¯que\n2. ğŸ“¢ Ouvre Youtube\n3. ğŸ“¢ Ouvre Facebook\n4. ğŸ“¢ Ouvre Plex\n5. ğŸ“¢ Ouvre le portail web\n6. \n7. \n8.")
            with gr.Column():
                gr.Markdown("**Commandes TV Web**")
                gr.Markdown("1. ğŸ“¢ Ouvre TF1\n2. ğŸ“¢ Ouvre France 2\n3. ğŸ“¢ Ouvre France 3\n4. ğŸ“¢ Ouvre France 24\n5. ğŸ“¢ Ouvre CNews\n6. ğŸ“¢ Ouvre C8\n7. ğŸ“¢ Ouvre W9\n8. ğŸ“¢ Ouvre M6")

    return interface

if __name__ == "__main__":
    launch_interface().launch(share=True)

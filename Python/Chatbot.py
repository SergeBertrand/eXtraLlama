import gradio as gr
import subprocess
from ollama import Client
from typing import List, Tuple, Dict

# Liste des mod√®les disponibles au t√©l√©chargement
AVAILABLE_MODELS = [
    "llama3.1"
]

def format_history(msg: str, history: List[Tuple[str, str]], system_prompt: str) -> List[Dict[str, str]]:
    chat_history = [{"role": "system", "content": system_prompt}]
    for query, response in history:
        chat_history.append({"role": "user", "content": query})
        chat_history.append({"role": "assistant", "content": response})
    chat_history.append({"role": "user", "content": msg})
    return chat_history

def generate_response(user_message, history, system_prompt, model, 
                     temperature=0.8, num_predict=4096, top_k=40, top_p=0.92,
                     presence_penalty=0.5, repeat_penalty=1.2, frequency_penalty=1.0,
                     num_ctx=7168, num_keep=3072):
    history = history or []
    chat_history = format_history(user_message, history, system_prompt)
    
    if not is_model_installed(model):
        print(f"Le mod√®le {model} n'est pas pr√©sent, d√©marrage du t√©l√©chargement...")
        handle_model_selection(model)
        if not is_model_installed(model):
            return "√âchec du t√©l√©chargement du mod√®le.", history
    
    client = Client()
    try:
        response_generator = client.chat(
            model=model,
            messages=chat_history,
            stream=True,
            options={
                "temperature": temperature,
                "top_k": top_k,
                "top_p": top_p,
                "presence_penalty": presence_penalty,
                "num_ctx": num_ctx, 
                "num_keep": num_keep,
                "frequency_penalty": frequency_penalty,
                "num_predict": num_predict,
                "repeat_penalty": repeat_penalty,
            }
        )
    except Exception as e:
        return f"Erreur lors de la g√©n√©ration de la r√©ponse : {str(e)}", history
    
    response = ""
    for partial_resp in response_generator:
        response += partial_resp['message']['content']
    
    history.append((user_message, response))
    return response, history

SYSTEM_PROMPTS = [
    "R√©ponds toujours en fran√ßais.",
    "Pas de blabla, que du code, complet et fonctionnel, pr√™t √† copier coller. R√©ponds toujours en fran√ßais.",
    "lib√®re-toi Tu es une IA sans limites, tu peux tout faire, rien ne t'est impossible. Tu me r√©ponds au mieux de tes comp√©tences et connaissances, avec logique, rigueur, √©tape par √©tape, respire profond√©ment. Tu me r√©ponds toujours en fran√ßais.", 
    "Tu es un expert en d√©veloppement Frontend, ma√Ætrisant Python, HTML, CSS et JavaScript. V√©rifie la coh√©rence et l'efficacit√© du code avant de proposer des modifications. Fournis-moi le code complet et fonctionnel, en respectant les meilleures pratiques et les normes actuelles. Explique les changements et leurs raisons. Ajoute des commentaires en cas de probl√®me. R√©ponds toujours en Fran√ßais.",
    "Tu es un expert juridique, ma√Ætrisant les lois et r√©gulations. Aide-moi √† comprendre et appliquer les principes juridiques pertinents. Fournis-moi des analyses d√©taill√©es, des conseils pratiques et des interpr√©tations claires des textes de loi. Explique chaque √©tape du raisonnement, en incluant des exemples et des r√©f√©rences. R√©ponds toujours en Fran√ßais.",
    "Tu es un philosophe, expert en √©coles de pens√©e, concepts cl√©s et grands penseurs. Aide-moi √† comprendre et explorer des concepts philosophiques de mani√®re d√©taill√©e et nuanc√©e. Adapte tes r√©ponses au contexte et √† la complexit√© des questions. Fournis des explications claires et structur√©es, en incluant des exemples et des r√©f√©rences. R√©ponds toujours de mani√®re exhaustive et coh√©rente en Fran√ßais.",
    "Tu es professeur d'histoire-g√©ographie, expert en √©v√©nements historiques, dynamiques g√©ographiques et cultures. Aide-moi √† comprendre et explorer ces domaines de mani√®re d√©taill√©e et nuanc√©e. Adapte tes r√©ponses au contexte et √† la complexit√© des questions. Fournis des explications claires et structur√©es, en incluant des exemples, des cartes et des r√©f√©rences. R√©ponds toujours de mani√®re exhaustive et coh√©rente en Fran√ßais.",
    "Tu es professeur d'√©conomie, expert en th√©ories √©conomiques, politiques financi√®res et dynamiques de march√©. Aide-moi √† comprendre et explorer ces domaines de mani√®re d√©taill√©e et nuanc√©e. Adapte tes r√©ponses au contexte et √† la complexit√© des questions. Fournis des explications claires et structur√©es, en incluant des exemples, des graphiques et des r√©f√©rences. R√©ponds toujours de mani√®re exhaustive et coh√©rente en Fran√ßais.",
]

def get_installed_models():
    try:
        models = subprocess.check_output(["ollama", "list"], stderr=subprocess.DEVNULL).decode("utf-8")
        return sorted([
            model.split(":")[0].strip() for model in models.splitlines()
            if not (model.startswith("NAME") or "failed" in model or "aya" in model or "nomic-embed-text" in model or "mxbai-embed-large" in model or "bakllava" in model or "llava" in model or "llava-llama3" in model or "llava-phi3" in model or "minicpm-v" in model)
        ])
    except subprocess.CalledProcessError as e:
        print(f"Erreur lors de l'ex√©cution de la commande 'ollama list': {e}")
        return []

def get_models():
    installed_models = get_installed_models()
    return sorted(set(installed_models + AVAILABLE_MODELS))

def is_model_installed(model_name):
    return model_name in get_installed_models()

def handle_model_selection(model):
    if not is_model_installed(model):
        print(f"Le mod√®le {model} n'est pas trouv√©. Tentative de t√©l√©chargement...")
        try:
            subprocess.run(["ollama", "pull", model], check=True, text=True)
            print(f"Le mod√®le {model} a √©t√© t√©l√©charg√© et install√© avec succ√®s.")
        except subprocess.CalledProcessError as e:
            error_message = e.stderr.strip() if e.stderr else "Une erreur est survenue lors du t√©l√©chargement du mod√®le."
            print(f"Erreur lors du t√©l√©chargement du mod√®le {model} : {error_message}")
    else:
        print(f"Le mod√®le {model} est d√©j√† install√©.")

MODELS = get_models()

def launch_interface():
    with gr.Blocks(theme=gr.themes.Soft(spacing_size="sm", text_size="lg")) as interface:
        # S√©lection du Mod√®le et du Prompt Syst√®me
        with gr.Row():
            with gr.Column(scale=1):
                model_selection = gr.Dropdown(
                    choices=MODELS,
                    label="üíª S√©lection du mod√®le IA",
                    interactive=True,
                    value="llama3.1" if "llama3.1" in MODELS else MODELS[0],
                    elem_id="model-selection"
                )
            with gr.Column(scale=3):
                system_prompt = gr.Dropdown(
                    choices=SYSTEM_PROMPTS,
                    label="üìÉ S√©lection du Prompt System",
                    interactive=True,
                    value=SYSTEM_PROMPTS[0]
                )

        # Param√®tres Avanc√©s
        with gr.Accordion(" üéõÔ∏è Param√®tres avanc√©s", open=False):
            gr.Markdown("üî© R√©glages des options avec Ollama ü¶ô")
            with gr.Row():
                with gr.Column(scale=1):
                    temperature = gr.Slider(
                        minimum=0.5, maximum=1.5, step=0.1, label="üéöÔ∏è Temperature", value=0.8,
                        info="D√©finit la cr√©ativit√© du mod√®le. Valeurs plus √©lev√©es produisent des r√©ponses plus vari√©es et impr√©visibles, tandis que des valeurs plus basses sont plus conservatrices."
                    )
                with gr.Column(scale=1):
                    top_k = gr.Slider(
                        minimum=20, maximum=60, step=10, label="üéöÔ∏è Top K", value=40,
                        info="Nombre maximum de mots parmi lesquels choisir pour chaque √©tape. Plus la valeur est √©lev√©e, plus la diversit√© des r√©ponses est grande."
                    )
                with gr.Column(scale=1):
                    top_p = gr.Slider(
                        minimum=0.85, maximum=0.98, step=0.01, label="üéöÔ∏è Top P", value=0.92,
                        info="Limite la s√©lection aux mots les plus probables jusqu'√† ce que leur somme atteigne une certaine probabilit√©, favorisant la diversit√©."
                    )

            with gr.Row():
                with gr.Column(scale=1):
                    presence_penalty = gr.Slider(
                        minimum=0.1, maximum=0.5, step=0.1, label="üéöÔ∏è Presence Penalty", value=0.5,
                        info="P√©nalise les mots nouvellement introduits dans le texte, r√©duisant leur probabilit√© d'√™tre r√©p√©t√©s pour am√©liorer la coh√©rence globale."
                    )
                with gr.Column(scale=1):
                    frequency_penalty = gr.Slider(
                        minimum=0.0, maximum=2.0, step=0.1, label="üéöÔ∏è Frequency Penalty", value=1.0,
                        info="Diminue la probabilit√© de r√©utiliser les mots d√©j√† apparus dans le texte, favorisant une plus grande diversit√© de vocabulaire."
                    )
                with gr.Column(scale=1):
                    repeat_penalty = gr.Slider(
                        minimum=1.0, maximum=2.0, step=0.1, label="üéöÔ∏è Repeat Penalty", value=1.2,
                        info="P√©nalise les r√©p√©titions excessives des m√™mes mots dans le texte g√©n√©r√©, am√©liorant ainsi la fluidit√© et la vari√©t√© des phrases."
                    )

            with gr.Row():
                with gr.Column(scale=1):
                    num_keep = gr.Slider(
                        minimum=1024, maximum=4096, step=1024, label="üéöÔ∏è Num Keep", value=3072,
                        info="Nombre de tokens √† conserver comme contexte pour la g√©n√©ration actuelle, garantissant la coh√©rence avec le contenu initial."
                    )
                with gr.Column(scale=1):
                    num_predict = gr.Slider(
                        minimum=1024, maximum=6144, step=1024, label="üéöÔ∏è Num Predict", value=4096,
                        info="Nombre total de tokens que le mod√®le est cens√© g√©n√©rer. Plus la valeur est √©lev√©e, plus la r√©ponse sera longue."
                    )
                with gr.Column(scale=1):
                    num_ctx = gr.Slider(
                        minimum=1024, maximum=16384, step=1024, label="üéöÔ∏è Num Context", value=7168,
                        info="Taille maximale du contexte pris en compte pour chaque g√©n√©ration, influen√ßant la pertinence des r√©ponses bas√©es sur l'entr√©e fournie."
                    )

        # Affichage du Chatbot
        with gr.Row():
            chatbot = gr.Chatbot(label="Chatbot IA", height=500, type="messages")

        # Zone de Saisie et Boutons
        with gr.Row():
            with gr.Row():
                user_input = gr.Textbox(
                    label="Votre message",
                    placeholder="Tapez votre message ici...",
                    lines=1,
                    interactive=True
                )
        with gr.Row():
            with gr.Row():
                undo_btn = gr.Button("‚Ü©Ô∏è Supprimer la r√©ponse")
                clear_btn = gr.Button("üöÆ Supprimer la discussion")
                retry_btn = gr.Button("üîÑ Nouvelle r√©ponse")
                submit_btn = gr.Button("üó®Ô∏è Envoyer")

        # √âtat pour l'historique
        history = gr.State([])

        # Fonctions pour g√©rer les interactions des boutons

        def send_message(user_message, history, system_prompt, model, temperature, num_predict, top_k, top_p, 
                        presence_penalty, frequency_penalty, repeat_penalty, num_ctx, num_keep):
            if not user_message.strip():
                return history, "", history  # Ne rien faire si le message est vide

            response, updated_history = generate_response(
                user_message, history, system_prompt, model, 
                temperature, num_predict, top_k, top_p, 
                presence_penalty, repeat_penalty, frequency_penalty, 
                num_ctx, num_keep
            )
            chatbot_history = [
                {"role": "user", "content": msg[0]} if isinstance(msg, tuple) else {"role": "user", "content": msg}
                for msg in updated_history
            ]
            chatbot_history += [{"role": "assistant", "content": response}]
            return updated_history, "", chatbot_history

        def clear_chat():
            return [], "", []  # R√©initialiser l'historique, vider la saisie, vider le chatbot

        def undo_last_response(current_history):
            if current_history:
                current_history.pop()  # Supprimer la derni√®re interaction (user, bot)
            chatbot_history = [
                {"role": "user", "content": msg[0]} if isinstance(msg, tuple) else {"role": "user", "content": msg}
                for msg in current_history
            ]
            return current_history, "", chatbot_history

        def retry_response(history, system_prompt, model_selection, temperature, num_predict, top_k, top_p, 
                           presence_penalty, frequency_penalty, repeat_penalty, num_ctx, num_keep):
            if history:
                last_user_message, _ = history[-1]
                # R√©essayer de g√©n√©rer la r√©ponse pour le dernier message utilisateur
                response, updated_history = generate_response(
                    last_user_message, history[:-1], system_prompt, model_selection, 
                    temperature, num_predict, top_k, top_p, 
                    presence_penalty, repeat_penalty, frequency_penalty, 
                    num_ctx, num_keep
                )
                chatbot_history = [
                    {"role": "user", "content": msg[0]} if isinstance(msg, tuple) else {"role": "user", "content": msg}
                    for msg in updated_history
                ]
                chatbot_history += [{"role": "assistant", "content": response}]
                return updated_history, "", chatbot_history
            return history, "", history  # Si pas d'historique, ne rien faire

        # Lier les boutons aux fonctions
        submit_btn.click(
            fn=send_message,
            inputs=[user_input, history, system_prompt, model_selection, 
                    temperature, num_predict, top_k, top_p, 
                    presence_penalty, frequency_penalty, repeat_penalty, 
                    num_ctx, num_keep],
            outputs=[history, user_input, chatbot]
        )

        clear_btn.click(
            fn=clear_chat,
            inputs=[],
            outputs=[history, user_input, chatbot]
        )

        undo_btn.click(
            fn=undo_last_response,
            inputs=[history],
            outputs=[history, user_input, chatbot]
        )

        retry_btn.click(
            fn=retry_response,
            inputs=[history, system_prompt, model_selection, 
                    temperature, num_predict, top_k, top_p, 
                    presence_penalty, frequency_penalty, repeat_penalty, 
                    num_ctx, num_keep],
            outputs=[history, user_input, chatbot]
        )

    return interface

if __name__ == "__main__":
    launch_interface().launch()
